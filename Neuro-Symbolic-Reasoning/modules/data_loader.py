"""
Data Loader Module

Dataset-agnostic data loader for causal discovery pipeline.

Core features:
- Direct CSV -> OneHot matrix (no triples intermediate format)
- Efficient tensor-based representation
- Variable structure metadata for block operations
- Support for multiple datasets (ALARM, Tuebingen, etc.)
"""

import json
import pandas as pd
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Optional


class CausalDataLoader:
    """
    Dataset-agnostic data loader for causal discovery
    
    Key responsibilities:
    1. Load CSV data directly into (N, n_states) OneHot matrix
    2. Build variable-to-states mapping for block operations
    3. Provide metadata for other modules
    
    Supports:
    - Discrete data (one-hot encoded states)
    - Continuous data (raw values, treated as single state per variable)
    - Any dataset with proper metadata JSON
    """
    
    def __init__(self, data_path: str, metadata_path: str):
        """
        Args:
            data_path: Path to CSV file with observational data
            metadata_path: Path to JSON metadata with state mappings
                          (generated by metadata_generator.py)
        """
        self.data_path = Path(data_path)
        self.metadata_path = Path(metadata_path)
        
        # Load metadata first
        with open(self.metadata_path, 'r') as f:
            self.metadata = json.load(f)
        
        # Build variable structure
        self.var_structure = self._build_variable_structure()
        
        print("=" * 70)
        print("DATA LOADER INITIALIZED")
        print("=" * 70)
        print(f"Dataset: {self.metadata.get('dataset_name', 'Unknown')}")
        print(f"Variables: {self.var_structure['n_variables']}")
        print(f"Total states: {self.var_structure['n_states']}")
        print(f"Data format: {self.metadata.get('data_format', 'unknown')}")
    
    def _build_variable_structure(self) -> Dict:
        """
        Build comprehensive variable structure metadata
        
        Returns:
            Dictionary with:
            - n_variables: Number of variables (37)
            - n_states: Total number of states (105)
            - var_to_states: {var_name: [state_indices]}
            - state_to_var: {state_idx: var_name}
            - idx_to_state: {state_idx: state_name}
            - state_to_idx: {state_name: state_idx}
        """
        state_to_idx = {}
        idx_to_state = {}
        var_to_states = {}
        state_to_var = {}
        idx_counter = 0
        
        # Build state indices from metadata
        for var_name, state_mapping in self.metadata['state_mappings'].items():
            var_states = []
            
            # Sort by state code to ensure consistent ordering
            sorted_codes = sorted(state_mapping.keys(), key=lambda x: int(x))
            
            for state_code in sorted_codes:
                state_name = state_mapping[state_code]
                
                # Map state to index
                state_to_idx[state_name] = idx_counter
                idx_to_state[idx_counter] = state_name
                state_to_var[idx_counter] = var_name
                var_states.append(idx_counter)
                
                idx_counter += 1
            
            var_to_states[var_name] = var_states
        
        return {
            'n_variables': len(var_to_states),
            'n_states': idx_counter,
            'var_to_states': var_to_states,
            'state_to_var': state_to_var,
            'idx_to_state': idx_to_state,
            'state_to_idx': state_to_idx,
            'variable_names': sorted(var_to_states.keys())
        }
    
    def load_data(self) -> torch.Tensor:
        """
        Load observational data as OneHot matrix (dataset-agnostic)
        
        Supports:
        - Discrete data: One-hot encoded states (binary values)
        - Continuous data: Raw values -> discretize -> one-hot encode
        
        Returns:
            Tensor of shape (N_samples, n_states)
            - For discrete: Binary values, exactly n_variables ones per row
            - For continuous: One-hot encoded after discretization
        """
        print("\n" + "=" * 70)
        print("LOADING OBSERVATIONAL DATA")
        print("=" * 70)
        
        # Read CSV
        df = pd.read_csv(self.data_path)
        
        # Get state columns (exclude common ID columns)
        id_columns = ['sample_id', 'patient_id', 'subject_id', 'id', 'index']
        state_columns = [col for col in df.columns if col not in id_columns]
        
        # Check data format
        data_format = self.metadata.get('data_format', 'unknown')
        
        # CRITICAL: Handle continuous data with discretization + one-hot conversion
        if data_format == 'one_hot_csv' and len(state_columns) < self.var_structure['n_states']:
            # This is continuous data that needs discretization + one-hot encoding
            print(f"Detected continuous data (columns: {len(state_columns)}, expected states: {self.var_structure['n_states']})")
            print(f"Applying discretization + one-hot encoding...")
            
            from .data_preprocessor import DataPreprocessor
            
            # Get discretization parameters from metadata
            n_bins = self.metadata.get('n_bins', 5)
            strategy = self.metadata.get('discretization_strategy', 'quantile')
            
            # Step 1: Discretize continuous data
            preprocessor = DataPreprocessor(data_type='continuous', strategy=strategy)
            df_discrete, _ = preprocessor.fit_transform(df[state_columns], n_bins=n_bins, strategy=strategy)
            
            # Step 2: Convert to one-hot encoding
            # For each variable, create n_bins binary columns
            onehot_columns = []
            for var_name in state_columns:
                for bin_idx in range(n_bins):
                    # Create binary column: 1 if this sample is in this bin, 0 otherwise
                    col_name = f"{var_name}_bin{bin_idx}"
                    onehot_columns.append((df_discrete[var_name] == bin_idx).astype(int))
            
            # Combine into one-hot matrix
            df_onehot = pd.concat(onehot_columns, axis=1)
            df_onehot.columns = [f"{var}_bin{i}" for var in state_columns for i in range(n_bins)]
            
            print(f"[OK] Discretization + One-hot encoding complete")
            print(f"  Original shape: {df[state_columns].shape}")
            print(f"  One-hot shape: {df_onehot.shape}")
            
            # Convert to tensor
            data_matrix = torch.tensor(df_onehot.values, dtype=torch.float32)
            
        else:
            # Already in correct format (discrete one-hot)
            # Verify all states are present
            if len(state_columns) != self.var_structure['n_states']:
                print(f"WARNING: Expected {self.var_structure['n_states']} states, found {len(state_columns)}")
            
            # Convert to tensor directly
            data_matrix = torch.tensor(df[state_columns].values, dtype=torch.float32)
        
        # Verify data integrity
        n_samples = data_matrix.shape[0]
        
        print(f"\nSamples loaded: {n_samples}")
        print(f"Matrix shape: {data_matrix.shape}")
        
        # Verify one-hot structure
        states_per_sample = data_matrix.sum(dim=1)
        print(f"States per sample: {states_per_sample.mean().item():.1f} (expected: {self.var_structure['n_variables']})")
        print(f"Total facts: {int(data_matrix.sum().item())}")
        print(f"Matrix density: {data_matrix.mean().item() * 100:.2f}%")
        
        # Sanity check: each sample should have exactly n_variables states active
        if not torch.allclose(states_per_sample, torch.tensor(float(self.var_structure['n_variables'])), atol=0.1):
            print("WARNING: Some samples don't have exactly one state per variable")
            print(f"  Min: {states_per_sample.min().item()}")
            print(f"  Max: {states_per_sample.max().item()}")
        
        return data_matrix
    
    def get_variable_structure(self) -> Dict:
        """Return variable structure metadata"""
        return self.var_structure
    
    def get_variable_blocks(self) -> List[Dict]:
        """
        Generate block structure for Group Lasso
        
        Returns:
            List of block definitions:
            [
                {
                    'var_pair': (var_a, var_b),
                    'row_indices': [indices for var_a states],
                    'col_indices': [indices for var_b states]
                },
                ...
            ]
        """
        blocks = []
        var_names = self.var_structure['variable_names']
        
        for var_a in var_names:
            for var_b in var_names:
                if var_a == var_b:
                    continue  # Skip self-loops
                
                blocks.append({
                    'var_pair': (var_a, var_b),
                    'row_indices': self.var_structure['var_to_states'][var_a],
                    'col_indices': self.var_structure['var_to_states'][var_b]
                })
        
        return blocks
    
    def get_state_info(self, state_idx: int) -> Dict:
        """
        Get information about a specific state
        
        Args:
            state_idx: State index (0-104)
        
        Returns:
            Dictionary with state_name, variable, is_normal
        """
        state_name = self.var_structure['idx_to_state'][state_idx]
        var_name = self.var_structure['state_to_var'][state_idx]
        is_normal = 'Normal' in state_name
        
        return {
            'state_name': state_name,
            'variable': var_name,
            'is_normal': is_normal
        }


if __name__ == "__main__":
    # Test the data loader with ALARM dataset
    print("Testing CausalDataLoader with ALARM dataset\n")
    
    # Use absolute paths for testing
    base_dir = Path(__file__).parent.parent
    
    loader = CausalDataLoader(
        data_path=str(base_dir / 'data' / 'alarm' / 'alarm_data_10000.csv'),
        metadata_path=str(base_dir / 'data' / 'alarm' / 'metadata.json')
    )
    
    # Load data
    data = loader.load_data()
    print(f"\nData shape: {data.shape}")
    
    # Get variable structure
    var_struct = loader.get_variable_structure()
    print(f"\nVariables: {var_struct['n_variables']}")
    print(f"States: {var_struct['n_states']}")
    
    # Get blocks
    blocks = loader.get_variable_blocks()
    print(f"\nTotal blocks: {len(blocks)}")
    print(f"Expected: {var_struct['n_variables'] * (var_struct['n_variables'] - 1)}")
    
    # Test state info
    print(f"\nSample state info:")
    for i in [0, 10, 50]:
        info = loader.get_state_info(i)
        print(f"  State {i}: {info['state_name']} (var: {info['variable']}, normal: {info['is_normal']})")






